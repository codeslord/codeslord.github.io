---
layout: post
title: Finding text similarity using huggingface transformers
date: 2023-02-10 17:31
summary: Understanding the similarity between text using transformer embedding is interesting as well as necessary for many computational tasks
categories: General
---

<img title="a title" alt="LFS" src="https://i.imgur.com/5CbvY5x.jpg">


In this tutorial I am going to illustrate how you can set compare text for similarity using Cosine Similarity between transformer embeddings

>Prerequisites:
Python, PyTorch, Transformers


### Step 1:

```python
from transformers import BertTokenizer
```

```python
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
```


### Step 2:

```python
input1 = tokenizer(text1, padding=True, truncation=True, return_tensors='pt')
```

```python
input2 = tokenizer(text2, padding=True, truncation=True, return_tensors='pt')
```

### Step 3:

```python
from transformers import BertModel
```

```python
model = BertModel.from_pretrained('bert-base-uncased')
```

### Step 4:

```python
with torch.no_grad():
    out1 = model(**input1)
    out2 = model(**input2)
```

```python
output_embedding1 = out1.last_hidden_state.mean(dim=1)
```

```python
output_embedding2 = out2.last_hidden_state.mean(dim=1)
```

### Step 5:

```python
import torch.nn as nn
```

```python
cosine_similarity = nn.CosineSimilarity(dim=0)
```


```python
text_similarity = cosine_similarity(output_embedding1, output_embedding2)
```




