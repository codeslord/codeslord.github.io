---
layout: post
title: Finding Text Similarity using Huggingface Transformers and PyTorch
date: 2023-02-10 17:31
summary: This tutorial explains how to use the Hugging Face Transformers library and PyTorch to find the similarity between two texts. This is interesting as well as necessary for many computational tasks involving text such as information retrieval systems and question answering systems.
categories: General
---

<img title="a title" alt="LFS" src="https://i.imgur.com/mgfWtPz.png">


In this tutorial I am going to explain how you can set compare text for similarity using Cosine Similarity between transformer embeddings.

>Prerequisites:
Python, PyTorch, Transformers


### Step 1:

The first step is to import the *BertTokenizer* (or any tokenizer from transformers) class from the transformers library and create a tokenizer by calling the *from_pretrained()* method with the *'bert-base-uncased'* pre-trained model. You may use any transformer model instead of this and use it with zero or few changes. Here, The tokenizer is used to encode the text inputs in a format that can be used as input to the BERT model. If you are using a different tokenizer, make sure the model you are importing correspond to it.

```python
from transformers import BertTokenizer
```

```python
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
```


### Step 2:

In this step, the two input texts, text1 and text2, are tokenized using the tokenizer instance created in Step 1. The padding and truncation arguments are set to True and we are going to return pytorch tensors. We use padding and truncation to ensure that the input tensors have the same shape and can be passed to the BERT model.

```python
input1 = tokenizer(text1, padding=True, truncation=True, return_tensors='pt')
```

```python
input2 = tokenizer(text2, padding=True, truncation=True, return_tensors='pt')
```

### Step 3:

Next, we import the *BertModel* class from the transformers library (you can import it first, I am importing it here just to ensure better understanding) and create an instance of it by calling the *from_pretrained()* method with the *bert-base-uncased* pre-trained model as an argument. If you are using a different model, load the respective model that will accept the tokens made with the tokenizer as well. The model is used to encode the input tokens and obtain their embeddings.

```python
from transformers import BertModel
```

```python
model = BertModel.from_pretrained('bert-base-uncased')
```

### Step 4:

In this step, we pass the tokenized inputs created in Step 2 to the BERT model with the model we have loaded in the previous tep. The *with torch.no_grad()* context ensures that the computations performed do not have any gradients, as we are not training the model. The outputs of the model are then used to obtain the mean of the last hidden state, which represents the embedding of the entire input sequence.

```python
with torch.no_grad():
    out1 = model(**input1)
    out2 = model(**input2)
```

```python
output_embedding1 = out1.last_hidden_state.mean(dim=1)
```

```python
output_embedding2 = out2.last_hidden_state.mean(dim=1)
```

### Step 5:

Finally, we import the *nn.CosineSimilarity* class from PyTorch and create an instance of it. We then pass the embeddings obtained in Step 4 to the *cosine_similarity()* method of the nn.CosineSimilarity instance to obtain the cosine similarity between the two input texts, which represents their similarity.
Please note that you might need to adjust your *dim* argument based on the model/embedding. For example for BART model the dim will be 1.

```python
import torch.nn as nn
```

```python
cosine_similarity = nn.CosineSimilarity(dim=0)
```


```python
text_similarity = cosine_similarity(output_embedding1, output_embedding2)
```
When using the cosine similarity function from the torch.nn library, **a similarity score closer to one indicates higher similarity between the two texts, with a score of one indicating that the texts are identical.** However, if a different cosine similarity function such as the one from scipy is used, a similarity score of zero indicates identical texts, and the closer the score is to zero, the higher the similarity between the two texts.

And we are done! We have used Hugging Face Transformers and PyTorch to find the similarity between two sentences.


